---
title: 常用的数据统计和分析方法
category: 数据分析
tags: data
---

# 常用的数据统计和分析方法

## 假设检验

（Hypothesis Testing）

什么是假设检验？
假设检验是用来判断样本与样本，样本与总体的差异是由抽样误差引起还是本质差别造成的统计推断方法。

假设检验类似于一种证伪的方法（反证法），但是有不同于一般的反证法。

当我们想证明一件事情是正确的时，我们是没有办法证明的。

这时候，我们需要假设这件事情不正确，然后证明假设是错的。就可以说明我们想证明的结论是正确的了。

### 假设检验的基本思想

1.小概率原理
　　如果对总体的某种假设是真实的，那么不利于或不能支持这一假设的事件A（小概率事件）在一次试验中几乎不可能发生的；要是在一次试验中A竟然发生了，就有理由怀疑该假设的真实性，拒绝这一假设。

### 假设检验的原理
　　一般地说，对总体某项或某几项作出假设，然后根据样本对假设作出接受或拒绝的判断，这种方法称为假设检验。
　　假设检验使用了一种类似于“反证法”的推理方法，它的特点是：
　　（1）先假设总体某项假设成立，计算其会导致什么结果产生。若导致不合理现象产生，则拒绝原先的假设。若并不导致不合理的现象产生，则不能拒绝原先假设，从而接受原先假设。
　　（2）它又不同于一般的反证法。所谓不合理现象产生，并非指形式逻辑上的绝对矛盾，而是基于小概率原理：概率很小的事件在一次试验中几乎是不可能发生的，若发生了，就是不合理的。至于怎样才算是“小概率”呢？通常可将概率不超过0.05的事件称为“小概率事件”，也可视具体情形而取0.1或0.01等。在假设检验中常记这个概率为α，称为显著性水平。
　　
### 假设检验的种类

假设检验可分为正态分布检验、正态总体均值分布检验、非参数检验三类。

其中，正态总体均值分布检验包括z检验和t检验。

z检验针对大样本（一般来说，样本量大于30）平均值差异性检验方法。
t检验针对小样本检验总体参数。
F检验：用于2个或2个以上的样本均数差别的显著性检验。（方差分析）

具体以后再另行介绍。

#### F检验（方差分析法）

一个复杂的事物，其中往往有许多因素互相制约又互相依存。方差分析的目的是通过数据分析找出对该事物有显著影响的因素，各因素之间的交互作用，以及显著影响因素的最佳水平等。

经过方差分析若拒绝了检验假设，只能说明多个样本总体均数不相等或不全相等。若要得到各组均数间更详细的信息，应在方差分析的基础上进行多个样本均数的两两比较。

1. 多个样本均数间两两比较
	多个样本均数间两两比较常用q检验的方法，即Newman-kueuls法，其基本步骤为：建立检验假设-->样本均数排序-->计算q值-->查q界值表判断结果。
2. 多个实验组与一个对照组均数间两两比较
　　多个实验组与一个对照组均数间两两比较，若目的是减小第II类错误，最好选用最小显著差法（LSD法）；若目的是减小第I类错误，最好选用新复极差法，前者查t界值表，后者查q'界值表。


## 相关分析

用来评估两个变量之间的相关性的分析方法。

通常用相关系数 r 来表示相关程度，0~1表示正相关，值越大，相关性越强；-1~0为负相关，值越小，相关性约强。

有很多库可以用来计算相关系数，也可以在网上寻找 r 的计算公式。

可以通过散点图来直观的查看两个变量的关系。

## 分类

主要是处理输出类型为离散值的数据。

通过各种分类算法，求决策边界。并通过点与决策边界的距离，来为这个点赋予一个类标签，并进行**预测**。

评估方法是，查准率。

分类算法常用的有：朴素贝叶斯、SVMs、决策树等。
属于**监督分类算法**。

监督分类的意思是，我们有很多样本，而且我们了解样本的正确分类，也就是正确答案。

### 机器学习

机器学习就是用来解决这类问题的。
在机器学习中，我们把特征作为输入，然后尝试生成标签，（标签也就是答案）。
机器学习能生成决策面。  
决策面通常位于两个不同的类之间的某个位置。

机器学习要做的就是，获取数据（数据包括特征和正确标签，也就是训练集），将其转化成决策面（DS-decision surface）。

通过决策面，对新的数据特征进行预测，并给它打上标签，看他属于哪个分类。

### 机器学习常见问题

过拟合：过拟合是指使用的特征过多，使得拟合过程中太过于关注细枝末节，数据泛化的不好。
也就是数据在训练集上的准确率很高，接近100%；但它不知道怎么泛化新数据，在测试集上准确率第。

欠拟合：没有提取出有用的特征，使得在测试集和训练集上表现的都不好。

### 常用机器学习算法

1. 朴素贝叶斯
2. SVMs
3. 决策树
4. 其他

区别在下篇文字中介绍


## 回归分析

### 回归分析法适用情况

**回归分析法适用情况，输出是连续的。**

当输出是连续的值的情况下，我们可以用线性回归模型来对数据进行拟合。而拟合出来的回归方程，可以用来预测。

### 回归分析法作用

回归分析法可以用来**建立预测模型**，预测某些结果。

#### 定义

>回归分析是应用及其广泛的分析方法之一。它为描述和分析变量间的相关关系，揭示变量间的内在规律，并用于预测、控制等问题，提供了行之有效的方法。
>由于在现实世界中，许多变量（或通过适当变换的变量）之间都具有或近似具有线性相关关系，而线性回归分析方法简单、理论完整，因此线性回归模型常作为数据分析的首要模型。

#### 分类

1. 按自变量个数来分类
	2. 一元回归分析：有一个自变量
	3. 多元回归分析：有多个自变量
4. 按照自变量和因变量的相关关系来分类
	5. 线性回归
	6. 非线性回归

### 一元线性回归    

这里相当于 Y 的值在一定程度上是受 X 的值影响的，我们可以通过线性回归方程式来对 X，Y进行拟合。
$Y = β0 + β1*X $
β0 和 β1 是系数，$\epsilon$ 是误差。 
$\epsilon$ 表示实际值与预测值的差，即 $\epsilon = actual-predicted$

**执行线性回归时，我们要最大程度的降低误差平方和。**即根据已有数据，求出 β0 和 β1， 并使得所有点的 $\epsilon$^2 的和最小。

也就是说，最佳回归是最小化误差平方和的回归。

$minimizes Σ \epsilon^2$

其中 actual 来自实际点， predicted 来自你从回归中获得的公式，也就是，找到是的误差平方和最小的 β0 和 β1，即回归方程的斜率和截距。

用来最小化回归中的误差平方和的方法有两种：

1. 最小二乘法（OLS）（sklearn上面线性回归也是用的这种方法。）
2. 梯度下降法

#### 评估方法

如何评估回归模型的好坏：

1. 通过可视化来评估；
2. 可以查看线性回归产生的误差。
	3. 可以用误差平方和来评估
	4. 可以用 $r^2$ 来评估

差别是，$r^2$不受数据点的影响，而误差平方和会受到拟合的数据点数量的影响。

如果用最小误差平方和来评估拟合的好坏的话，会有一个问题，就是点越多，误差平方和不可避免的会越大，但是这并不能说明拟合的不好。
所以我们要选一个更好的评估指标。

#### 评估指标
$r^2$ ：R平方指标。范围为0~1，越接近1，表示回归线拟合的越好。
可以用sklearn里面的 linearRegression来求出 $r^2$ ，斜率和截距。
$r^2$ 的公式可以在网上查，这块暂时没有记录。


**对与多元回归分析**
$Y = β0 + β1*X1 + β2*X2 + …… + βn*Xn + \epsilon$

Y的值受到 X1, X2, ……, Xn 的影响。我们需要找到使误差平方和最小的各个系数。 

通过sklearn里面的linearRegresson模块，来进行回归线拟合，并用$r^2$来评估回归的好坏。

## 分类与回归比较

算法的输出类型 | 监督分类 | 回归
--------- | ------------- | -------------
output type（输出类型） | 类标签形式是离散的 | 连续的（基本上表明你将使用回归来预测数字）
在执行分类和回归时，你真正尝试寻找的是什么？ | 决策边界（根据点相对于决策边界的位置，为其赋予一个类标签） | 我们尝试找到的是“最优拟合线”，这是拟合数据的线条，而不是描述数据的边界
如何评估好坏 | 使用查准率作为指标 accuracy | 有两种评估方法：1. 误差平方和；2. $r^2$

回归和分类都是监督学习算法。
回归是不同类型的监督学习算法。

## 聚类分析（探索性分析方法）

属于非监督算法。
也就是我们获得的数据并不附有标记，也就不能告诉我们正确答案。
当数据不附有标签时，所有的数据都属于同一类别，我们仍可以通过一定的措施，来提取出有用的信息。

聚类分析的原则是：
同一类中的个体有较大的相似性，不同类的个体差异性很大。

1. 适用于没有先验知识的分类。如果没有这些事先的经验或一些国际标准、国内标准、行业标准，分类便会显得随意和主观。这时只要设定比较完善的分类变量，就可以通过聚类分析法得到较为科学合理的类别；
2. 可以处理多个变量决定的分类。例如，要根据消费者购买量的大小进行分类比较容易，但如果在进行数据挖掘时，要求根据消费者的购买量、家庭收入、家庭支出、年龄等多个指标进行分类通常比较复杂，而聚类分析法可以解决这类问题；
3. 聚类分析法是一种**探索性分析方法**，能够分析事物的内在特点和规律，并**根据相似性原则对事物进行分组**，是数据挖掘中常用的一种技术。


### K-means

使用最多的聚类算法是 K-means。算法核心如下：

1. 分配N个中心点
2. 优化
3. 迭代

算法的意思是，先分配N中心点，分别找到与中心点最近的点，根据中心点和最近的这些点的位置，对中心点进行优化，使中心点离这些点的距离最小；
根据新的中心点的位置重新划分离中心点最近的点，重复上述优化和分类过程，直到找到所有正确的中心点。

相当于人为的按照中心点，对数据进行了分类。

使得同一个类别里面，点的特性都是相似的；不同的类别里面，点的特性都是相异的。

### 应用

聚类分析适合对多个变量进行分类（多个维度）。

#### 可以用在客户细分上面。

消费同一种类的商品或服务时，不同的客户有不同的消费特点，通过研究这些特点，企业可以制定出不同的营销组合，从而获取最大的消费者剩余，这就是客户细分的主要目的。

#### 在实验单元选择中的应用

在实验调查方法中，最常用的是前后单组对比实验、对照组对比实验和前后对照组对比实验。

这些方法要求实验组和对照组的单元具有可比性，即两类单元的主客观条件基本一致。

通过聚类分析，可将待选的实验市场（商场、居民区、城市等）分成同质的几类小组，在同一组内选择实验单位和非实验单位，这样便保证了这两个单位之间具有了一定的可比性。

聚类时，商店的规模、类型、设备状况、所处的地段、管理水平等就是聚类的分析变量。聚类分析在抽样方案设计中的应用。

#### 在销售片区确定中的应用

#### 在市场机会研究中的应用

其实就是将多种变量作为分析变量，例如产品、品牌知名度、等等，通过分析，可以知道企业和别的哪些企业形成了直接和间接的竞争关系。

聚类以后属于同一类别的产品和品牌就是所分析企业的直接竞争对手。
在制定战略时，可以更多的运用“红海战略”。

在聚类以后，结合每一产品或品牌的多种不同属性的研究，可以发现哪些属性组合目前还没有融入产品或品牌中，从而寻找企业在市场中的机会，为企业制定合理的“蓝海战略”提供基础性的资料。

## 主成分分析

用降维的思想，把多个指标转化成少量的综合指标。

这里探讨的是，在机器学习中，对众多的特征进行降维，找到最重要的一些特征，然后进行学习。

一个数据集中的第一主成分，是数据集中，具有最大方差的方向。

这个变换把数据变换到一个新的坐标系统中，使得任何数据投影的第一大方差在第一个坐标(称为第一主成分)上，第二大方差在第二个坐标(第二主成分)上，依次类推。主成分分析经常用减少数据集的维数，同时保持数据集的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。


